{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f258658b-95e1-4cef-8ade-b0b5f5ec1504"
    }
   },
   "source": [
    "## The `model.py` file\n",
    "This will be used to create the model which will be used along with keras, the main purpose of this file is to train the model using the data saved from our preprocessed file,\n",
    "Once the training is done the model and weights are saved in model.h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "c7931b50-cd60-406f-afb1-e1745a7a8072"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, ELU, Flatten, Dropout, Dense, Lambda, MaxPooling2D\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f546d22f-1568-42b8-9089-ad7d15475010"
    }
   },
   "source": [
    "## Preprocessing\n",
    "The preprocessing is done with these helper functions: change_brightness, resize_to_target_size and crop_and_resize\n",
    "This will help change the images in a way that will best suite the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "612cbeb6-aec4-4166-ba77-4885978f94c9"
    }
   },
   "outputs": [],
   "source": [
    "rows, cols, ch = 64, 64, 3\n",
    "TARGET_SIZE = (64, 64)\n",
    "\n",
    "\n",
    "def change_brightness(image):\n",
    "    image1 = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # randomly generate the brightness reduction factor\n",
    "    # The 0.25 used is to prevent the image to be completely dark\n",
    "    random_bright = .25+np.random.uniform()\n",
    "\n",
    "    # Apply the brightness reduction\n",
    "    image1[:,:,2] = image1[:,:,2]*random_bright\n",
    "\n",
    "    # convert to RBG again\n",
    "    image1 = cv2.cvtColor(image1,cv2.COLOR_HSV2RGB)\n",
    "    return image1\n",
    "\n",
    "\n",
    "def resize_to_target_size(image):\n",
    "    return cv2.resize(image, TARGET_SIZE)\n",
    "\n",
    "\n",
    "def crop_and_resize(image):\n",
    "    # The input image of dimensions 160x320x3\n",
    "    # Output image of size 64x64x3\n",
    "    cropped_image = image[55:135, :, :]\n",
    "    processed_image = resize_to_target_size(cropped_image)\n",
    "    return processed_image\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = crop_and_resize(image)\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    # Normalize image\n",
    "    image = image/255.0 - 0.5\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_augmented_row(row):\n",
    "    steering = row['steering']\n",
    "    camera = np.random.choice(['center', 'left', 'right'])\n",
    "\n",
    "    # change the steering angle relative to camera\n",
    "    if camera == 'left':\n",
    "        steering += 0.25\n",
    "    elif camera == 'right':\n",
    "        steering -= 0.25\n",
    "\n",
    "    image = load_img(\"data/\" + row[camera].strip())\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    # Horizontally flip the image, helps to reduce left bias\n",
    "    flip_prob = np.random.random()\n",
    "    if flip_prob > 0.5:\n",
    "        steering = -1*steering\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "    # Apply brightness augmentation\n",
    "    image = change_brightness(image)\n",
    "    image = preprocess_image(image)\n",
    "    return image, steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3c758359-111c-4fe1-aed7-4f5727a6d018"
    }
   },
   "source": [
    "## The model proper\n",
    "The model is initialized and trained with the parameters defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "2e15bec4-0c02-4b8d-9943-ab5a02758b99"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 32, 32, 32)    2432        convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "elu_1 (ELU)                      (None, 32, 32, 32)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 30, 30, 16)    4624        elu_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_2 (ELU)                      (None, 30, 30, 16)    0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 30, 30, 16)    0           elu_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 15, 15, 16)    0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 13, 13, 16)    2320        maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "elu_3 (ELU)                      (None, 13, 13, 16)    0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 13, 13, 16)    0           elu_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 2704)          0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1024)          2769920     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 1024)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "elu_4 (ELU)                      (None, 1024)          0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 512)           524800      elu_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_5 (ELU)                      (None, 512)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             513         elu_5[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 3,304,609\n",
      "Trainable params: 3,304,609\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 514s - loss: 0.0905 - val_loss: 2.0838e-04\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 367s - loss: 0.0340 - val_loss: 2.6477e-04\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 249s - loss: 0.0305 - val_loss: 7.7173e-04\n",
      "Saving model weights and configuration file.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def get_data_generator(data_frame, batch_size=32):\n",
    "    N = data_frame.shape[0]\n",
    "    batches_per_epoch = N // batch_size\n",
    "\n",
    "    i = 0\n",
    "    while(True):\n",
    "        start = i*batch_size\n",
    "        end = start+batch_size - 1\n",
    "\n",
    "        X_batch = np.zeros((batch_size, 64, 64, 3), dtype=np.float32)\n",
    "        y_batch = np.zeros((batch_size,), dtype=np.float32)\n",
    "\n",
    "        j = 0\n",
    "\n",
    "        # slice a `batch_size` sized chunk from the data and generate augmented\n",
    "        # images directly\n",
    "        for index, row in data_frame.loc[start:end].iterrows():\n",
    "            X_batch[j], y_batch[j] = get_augmented_row(row)\n",
    "            j += 1\n",
    "\n",
    "        i += 1\n",
    "        if i == batches_per_epoch - 1:\n",
    "            # reset the index so that we can cycle over the data_frame again\n",
    "            i = 0\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    # initialize the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1 shape is 32x32x32\n",
    "    model.add(Convolution2D(32, 5, 5, input_shape=(64, 64, 3), subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "\n",
    "    # 2 shape is 15x15x16\n",
    "    model.add(Convolution2D(16, 3, 3, subsample=(1, 1), border_mode=\"valid\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(MaxPooling2D((2, 2), border_mode='valid'))\n",
    "\n",
    "    # 3 shape is 12x12x16\n",
    "    model.add(Convolution2D(16, 3, 3, subsample=(1, 1), border_mode=\"valid\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Dropout(.4))\n",
    "\n",
    "    # Flatten the output\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # 4\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(ELU())\n",
    "\n",
    "    # 5\n",
    "    model.add(Dense(512))\n",
    "    model.add(ELU())\n",
    "\n",
    "    # Single output due to the regression\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Adam optimizer\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    # Print out the model\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    # Get the data with labels from the csv file\n",
    "    data_frame = pd.read_csv('data/driving_log.csv', usecols=[0, 1, 2, 3])\n",
    "    \n",
    "    # Dividing the data\n",
    "    data_frame = data_frame.sample(frac=1).reset_index(drop=True)\n",
    "    training_split = 0.8\n",
    "\n",
    "    num_rows_training = int(data_frame.shape[0]*training_split)\n",
    "\n",
    "    training_data = data_frame.loc[0:num_rows_training-1]\n",
    "    validation_data = data_frame.loc[num_rows_training:]\n",
    "\n",
    "    # Memory free\n",
    "    data_frame = None\n",
    "\n",
    "    training_generator = get_data_generator(training_data, batch_size=BATCH_SIZE)\n",
    "    validation_data_generator = get_data_generator(validation_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = get_model()\n",
    "\n",
    "    # train 20000 samples in each epoch\n",
    "    samples_per_epoch = (20000//BATCH_SIZE)*BATCH_SIZE\n",
    "\n",
    "    model.fit_generator(training_generator, validation_data=validation_data_generator,\n",
    "                        samples_per_epoch=samples_per_epoch, nb_epoch=3, nb_val_samples=3000)\n",
    "\n",
    "    print(\"Saving model weights and configuration file.\")\n",
    "\n",
    "    model.save_weights('model.h5')\n",
    "    with open('model.json', 'w') as outfile:\n",
    "        outfile.write(model.to_json())\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "407ea407-d035-4054-8f04-63d99775abf0"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "7a7bc708-3620-45f4-8440-6a8b047fa1a8",
    "theme": {
     "7a7bc708-3620-45f4-8440-6a8b047fa1a8": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "7a7bc708-3620-45f4-8440-6a8b047fa1a8",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         17,
         17,
         17
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         238,
         238
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         231,
         173,
         82
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         238,
         238,
         238
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Montserrat",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Montserrat",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Montserrat",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Montserrat",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Montserrat"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Montserrat"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Montserrat"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Open Sans",
        "font-size": 4
       },
       "p": {
        "color": "mainColor",
        "font-family": "Open Sans",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Open Sans",
       "font-size": 4
      }
     }
    }
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
